"""
Reflection ë…¸ë“œ - ë©”ëª¨ë¦¬ ë°˜ì˜ + í™•ì‹ ë„ ê³„ì‚°

[ì—­í• ]
- ì¥ê¸° ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
- ë‹¨ê¸° ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
- ì´ˆì•ˆì— ë©”ëª¨ë¦¬ ë°˜ì˜
- í™•ì‹ ë„ ê³„ì‚° (rerank í•„ìš” ì—¬ë¶€ íŒë‹¨)
"""

from typing import List, Dict, Any
from core.state import AgentState


def reflection_node(state: AgentState) -> dict:
    """
    Reflection ë…¸ë“œ
    
    Input: final_response, short_memory, long_memory, retrieved_docs
    Output: refined_response, confidence_score, need_rerank
    """
    
    query = state["user_query"]
    response = state.get("final_response", "")
    user_id = state["user_id"]
    
    # 1. ì¥ê¸° ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
    relevant_memories = search_long_memory(query, user_id)
    
    # 2. ë‹¨ê¸° ë©”ëª¨ë¦¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
    short_context = get_short_context(state.get("short_memory", []))
    
    # 3. ë©”ëª¨ë¦¬ ë°˜ì˜í•˜ì—¬ ì‘ë‹µ ê°œì„ 
    refined_response = refine_response(
        original=response,
        memories=relevant_memories,
        context=short_context
    )
    
    # 4. í™•ì‹ ë„ ê³„ì‚°
    confidence = calculate_confidence(state)
    
    # 5. rerank í•„ìš” ì—¬ë¶€
    need_rerank = confidence < 0.7
    
    return {
        "final_response": refined_response,
        "confidence_score": confidence,
        "need_rerank": need_rerank,
        "current_step": "reflection_done"
    }


def search_long_memory(query: str, user_id: str, top_k: int = 3) -> List[Dict]:
    """ì¥ê¸° ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰"""
    try:
        from rag.embedder import get_embedding
        from rag.vector_db import search_similar
        
        embedding = get_embedding(query)
        
        results = search_similar(
            embedding=embedding,
            filter={"user_id": user_id},
            top_k=top_k
        )
        
        return results or []
    except Exception as e:
        print(f"[reflection] ì¥ê¸° ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []


def get_short_context(short_memory: List[Dict]) -> str:
    """ë‹¨ê¸° ë©”ëª¨ë¦¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not short_memory:
        return ""
    
    # ìµœê·¼ 3í„´ë§Œ
    recent = short_memory[-3:]
    
    context_parts = []
    for turn in recent:
        context_parts.append(f"Q: {turn.get('query', '')}")
        context_parts.append(f"A: {turn.get('response', '')[:100]}")
    
    return "\n".join(context_parts)


def refine_response(
    original: str,
    memories: List[Dict],
    context: str
) -> str:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ ê°œì„ """
    
    if not memories and not context:
        return original
    
    # TODO: LLMìœ¼ë¡œ ê°œì„ 
    # ì§€ê¸ˆì€ ê°„ë‹¨íˆ ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
    
    additions = []
    
    for mem in memories:
        mem_type = mem.get("metadata", {}).get("type", "")
        content = mem.get("metadata", {}).get("content", "")
        
        if mem_type == "allergy":
            additions.append(f"âš ï¸ ì°¸ê³ : {content}")
        elif mem_type == "preference":
            additions.append(f"ğŸ’¡ ì„ í˜¸ë„ ë°˜ì˜: {content}")
    
    if additions:
        return original + "\n\n" + "\n".join(additions)
    
    return original


def calculate_confidence(state: AgentState) -> float:
    """
    í™•ì‹ ë„ ê³„ì‚°
    
    [ê³ ë ¤ ìš”ì†Œ]
    - RAG ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ
    - ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
    - ì¶œì²˜ ë‹¤ì–‘ì„±
    """
    
    score = 0.5  # ê¸°ë³¸ê°’
    
    # RAG ê²°ê³¼
    docs = state.get("retrieved_docs", [])
    if docs:
        avg_score = sum(d.get("score", 0) for d in docs) / len(docs)
        score += avg_score * 0.3
    
    # ê²€ìƒ‰ ê²°ê³¼
    search_results = state.get("search_results", [])
    if search_results:
        score += min(len(search_results) / 10, 0.2)
    
    # ì œì•½ì¡°ê±´ ìœ„ë°˜ ì—†ìŒ
    if not state.get("constraint_violations"):
        score += 0.1
    
    return min(score, 1.0)