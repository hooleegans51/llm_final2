"""
ë©”ì¸ ì—ì´ì „íŠ¸ ë…¸ë“œ

[ì—­í• ]
1. 1ì°¨ LLM í˜¸ì¶œ: ì´ˆì•ˆ + ë„êµ¬ íŒë‹¨ (ReAct Thought/Action)
2. 2ì°¨ LLM í˜¸ì¶œ: ë„êµ¬ ê²°ê³¼ ë°˜ì˜ â†’ ìµœì¢… ë‹µë³€
3. System Interrupt ë°œìƒ (ì˜ˆì‚° ì´ˆê³¼ ë“±)

[íë¦„]
1ì°¨ LLM â†’ (ë„êµ¬ í•„ìš”ì‹œ) tool_node â†’ 2ì°¨ LLM â†’ ì™„ë£Œ
"""

import os
import json
from typing import Dict, Any

from dotenv import load_dotenv
from langgraph.types import interrupt

from core.state import AgentState, ReActStep, record_llm_call
from core.prompts import (
    build_first_llm_prompt,
    build_second_llm_prompt,
    build_react_prompt,
    parse_react_response,
    format_search_results
)

load_dotenv()

# ============================================================
# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# ============================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")

# OpenAI í´ë¼ì´ì–¸íŠ¸
_client = None

def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    global _client
    if _client is None and OPENAI_API_KEY:
        try:
            from openai import OpenAI
            _client = OpenAI(api_key=OPENAI_API_KEY)
        except Exception as e:
            print(f"[main_agent] OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    return _client


def call_llm(system_prompt: str, user_prompt: str, json_mode: bool = False) -> str:
    """
    OpenAI LLM í˜¸ì¶œ
    
    Args:
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        json_mode: JSON ì‘ë‹µ ëª¨ë“œ ì—¬ë¶€
    
    Returns:
        LLM ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    client = get_openai_client()
    
    if client is None:
        raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    kwargs = {
        "model": CHAT_MODEL,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 2000
    }
    
    if json_mode:
        kwargs["response_format"] = {"type": "json_object"}
    
    response = client.chat.completions.create(**kwargs)
    return response.choices[0].message.content


# ============================================================
# ë©”ì¸ ì—ì´ì „íŠ¸ ë…¸ë“œ
# ============================================================

def main_agent_node(state: AgentState) -> dict:
    """
    ë©”ì¸ ì—ì´ì „íŠ¸ ë…¸ë“œ
    
    [ë‹¨ê³„]
    1. Interrupt ì²´í¬/ì²˜ë¦¬
    2. 1ì°¨ LLM (ì´ˆì•ˆ + ë„êµ¬ íŒë‹¨)
    3. 2ì°¨ LLM (ë„êµ¬ ê²°ê³¼ ë°˜ì˜)
    """
    
    # =========================================
    # 1. System Interrupt ì²´í¬
    # =========================================
    if state["constraint_violations"] and not state["user_interrupt_response"]:
        violation = state["constraint_violations"][0]
        
        # interrupt() í˜¸ì¶œ â†’ ê·¸ë˜í”„ ë©ˆì¶¤
        user_choice = interrupt({
            "type": violation["type"],
            "message": f"ì˜ˆì‚°ì„ {violation.get('diff', 0):,}ì› ì´ˆê³¼í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ í• ê¹Œìš”?",
            "options": ["ê³„ì† ì§„í–‰", "ì €ë ´í•œ ëŒ€ì•ˆ ì°¾ê¸°", "ì·¨ì†Œ"]
        })
        
        return {
            "user_interrupt_response": user_choice,
            "current_step": "interrupt_resolved"
        }
    
    # =========================================
    # 2. Interrupt ì‘ë‹µ ì²˜ë¦¬
    # =========================================
    if state["user_interrupt_response"]:
        return handle_interrupt_response(state)
    
    # =========================================
    # 3. ìµœëŒ€ ë°˜ë³µ ì²´í¬ (ReAct ë¬´í•œë£¨í”„ ë°©ì§€)
    # =========================================
    if state["iteration_count"] >= state["max_iterations"]:
        return {
            "final_response": generate_forced_answer(state),
            "current_step": "max_iteration_reached"
        }
    
    # =========================================
    # 4. 1ì°¨ LLM í˜¸ì¶œ (ì•„ì§ ì•ˆ í–ˆìœ¼ë©´)
    # =========================================
    if not state["llm_1st_response"]:
        return call_first_llm(state)
    
    # =========================================
    # 5. 2ì°¨ LLM í˜¸ì¶œ (ë„êµ¬ ê²°ê³¼ ìˆìœ¼ë©´)
    # =========================================
    if state["search_results"] and not state["llm_2nd_response"]:
        return call_second_llm(state)
    
    # =========================================
    # 6. ë„êµ¬ ë¶ˆí•„ìš” â†’ 1ì°¨ ì‘ë‹µì´ ìµœì¢…
    # =========================================
    if state["llm_1st_response"] and not state["need_web_search"]:
        return {
            "final_response": state["llm_1st_response"],
            "current_step": "complete"
        }
    
    # ë„êµ¬ ì‹¤í–‰ ëŒ€ê¸° ì¤‘
    return {"current_step": "waiting_tool"}


# ============================================================
# Interrupt ì²˜ë¦¬
# ============================================================

def handle_interrupt_response(state: AgentState) -> dict:
    """Interrupt ì‘ë‹µ ì²˜ë¦¬"""
    response = state["user_interrupt_response"]
    
    if response == "ì·¨ì†Œ":
        return {
            "final_response": "ì‘ì—…ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.",
            "current_step": "cancelled"
        }
    
    elif response == "ì €ë ´í•œ ëŒ€ì•ˆ ì°¾ê¸°":
        # ìƒíƒœ ë¦¬ì…‹í•˜ê³  ëŒ€ì•ˆ ì°¾ê¸°
        return {
            "llm_1st_response": "",
            "llm_2nd_response": "",
            "search_results": [],
            "constraint_violations": [],
            "user_interrupt_response": None,
            "need_web_search": True,
            "search_queries": [f"{state['user_query']} ì €ë ´í•œ ëŒ€ì•ˆ"],
            "current_thought": "ì‚¬ìš©ìê°€ ì €ë ´í•œ ëŒ€ì•ˆì„ ì›í•œë‹¤. ë” ì €ë ´í•œ ì˜µì…˜ì„ ì°¾ì•„ë³´ì.",
            "iteration_count": state["iteration_count"] + 1,
            "current_step": "finding_alternative"
        }
    
    else:  # "ê³„ì† ì§„í–‰"
        return {
            "user_interrupt_response": None,
            "constraint_violations": [],
            "current_step": "continue"
        }


# ============================================================
# 1ì°¨ LLM í˜¸ì¶œ
# ============================================================

def call_first_llm(state: AgentState) -> dict:
    """
    1ì°¨ LLM í˜¸ì¶œ
    - ì´ˆì•ˆ ìƒì„±
    - ë„êµ¬ í•„ìš” íŒë‹¨ (ReAct: Thought â†’ Action)
    """
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    system_prompt, user_prompt = build_first_llm_prompt(
        user_query=state["user_query"],
        constraints=state.get("user_constraints", {}),
        rag_results=state.get("retrieved_docs", [])
    )
    
    # JSON ì‘ë‹µì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    json_instruction = """

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "draft": "ì´ˆì•ˆ ë‹µë³€ ë‚´ìš©",
    "need_tools": true ë˜ëŠ” false,
    "thought": "í˜„ì¬ ìƒê° ê³¼ì •",
    "action": "ì‚¬ìš©í•  ë„êµ¬ ì´ë¦„ (shopping_search, recipe_search, calorie, weather ë“±) ë˜ëŠ” null",
    "action_input": "ë„êµ¬ì— ì „ë‹¬í•  ì…ë ¥ê°’",
    "tool_queries": ["ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸"]
}

ë„êµ¬ ëª©ë¡:
- shopping_search: ì¬ë£Œ ê°€ê²© ê²€ìƒ‰
- recipe_search: ë ˆì‹œí”¼ ê²€ìƒ‰
- calorie: ì¹¼ë¡œë¦¬ ì •ë³´
- weather: ë‚ ì”¨ ì •ë³´
- health_guidelines: ê±´ê°•/ì§ˆë³‘ ê´€ë ¨ ì •ë³´
"""
    
    try:
        # ì‹¤ì œ LLM í˜¸ì¶œ
        response = call_llm(
            system_prompt=system_prompt,
            user_prompt=user_prompt + json_instruction,
            json_mode=True
        )
        result = json.loads(response)
        
    except Exception as e:
        print(f"[main_agent] 1ì°¨ LLM ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        result = {
            "draft": f"'{state['user_query']}'ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "need_tools": False,
            "thought": "LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "action": None,
            "action_input": "",
            "tool_queries": []
        }
    
    # ReAct ìŠ¤í… ìƒì„±
    new_step: ReActStep = {
        "thought": result.get("thought", "ì´ˆì•ˆì„ ì‘ì„±í•˜ê³  ë„êµ¬ í•„ìš” ì—¬ë¶€ë¥¼ íŒë‹¨í•œë‹¤."),
        "action": result.get("action"),
        "action_input": result.get("action_input", ""),
        "observation": None
    }
    
    # State ì—…ë°ì´íŠ¸
    updates = {
        "llm_1st_response": result.get("draft", ""),
        "need_web_search": result.get("need_tools", False),
        "search_queries": result.get("tool_queries", []),
        "current_thought": new_step["thought"],
        "current_action": result.get("action"),
        "current_action_input": result.get("action_input", ""),
        "react_steps": [new_step],
        "iteration_count": state["iteration_count"] + 1,
        "current_step": "1st_llm_done",
        
        # LLM í˜¸ì¶œ ê¸°ë¡
        **record_llm_call(
            state,
            call_type="1st_llm",
            node_name="main_agent",
            input_summary=f"Query: {state['user_query'][:50]}",
            output_summary=f"Draft: {result.get('draft', '')[:50]}"
        )
    }
    
    return updates


# ============================================================
# 2ì°¨ LLM í˜¸ì¶œ
# ============================================================

def call_second_llm(state: AgentState) -> dict:
    """
    2ì°¨ LLM í˜¸ì¶œ
    - ë„êµ¬ ê²°ê³¼ ë°˜ì˜
    - ìµœì¢… ë‹µë³€ ìƒì„±
    """
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    system_prompt, user_prompt = build_second_llm_prompt(
        draft=state["llm_1st_response"],
        search_results=state.get("search_results", []),
        constraints=state.get("user_constraints", {})
    )
    
    try:
        # ì‹¤ì œ LLM í˜¸ì¶œ
        final_answer = call_llm(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            json_mode=False
        )
        
    except Exception as e:
        print(f"[main_agent] 2ì°¨ LLM ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        final_answer = format_fallback_response(state)
    
    # ReAct ìŠ¤í… ì—…ë°ì´íŠ¸ (FINISH)
    finish_step: ReActStep = {
        "thought": "ë„êµ¬ ê²°ê³¼ë¥¼ ë°›ì•˜ë‹¤. ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì.",
        "action": "FINISH",
        "action_input": final_answer[:100],
        "observation": None
    }
    
    # State ì—…ë°ì´íŠ¸
    updates = {
        "llm_2nd_response": final_answer,
        "final_response": final_answer,
        "react_steps": [finish_step],
        "current_thought": finish_step["thought"],
        "current_step": "2nd_llm_done",
        
        # LLM í˜¸ì¶œ ê¸°ë¡
        **record_llm_call(
            state,
            call_type="2nd_llm",
            node_name="main_agent",
            input_summary=f"Search results: {len(state.get('search_results', []))}ê°œ",
            output_summary=f"Final: {final_answer[:50]}"
        )
    }
    
    return updates


def format_fallback_response(state: AgentState) -> str:
    """LLM ì˜¤ë¥˜ ì‹œ í´ë°± ì‘ë‹µ ìƒì„±"""
    query = state["user_query"]
    search_results = state.get("search_results", [])
    constraints = state.get("user_constraints", {})
    budget = constraints.get("budget", 0)
    
    # ê°€ê²© í•©ê³„ ê³„ì‚°
    total_price = sum(
        item.get("price", 0) 
        for item in search_results 
        if item.get("type") == "shopping"
    )
    
    response = f"## {query} ê²°ê³¼\n\n"
    
    # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
    if search_results:
        shopping_items = [r for r in search_results if r.get("type") == "shopping"]
        recipe_items = [r for r in search_results if r.get("type") == "recipe"]
        
        if shopping_items:
            response += "### ğŸ›’ ì¬ë£Œ ëª©ë¡\n"
            for item in shopping_items[:5]:
                response += f"- {item.get('title', 'ìƒí’ˆ')}: {item.get('price', 0):,}ì›\n"
            response += f"\n**ì´ ì˜ˆìƒ ë¹„ìš©: {total_price:,}ì›**\n"
            
            if budget:
                if total_price <= budget:
                    response += f"âœ… ì˜ˆì‚° {budget:,}ì› ë‚´ì…ë‹ˆë‹¤.\n"
                else:
                    response += f"âš ï¸ ì˜ˆì‚° {budget:,}ì›ì„ {total_price - budget:,}ì› ì´ˆê³¼í•©ë‹ˆë‹¤.\n"
        
        if recipe_items:
            response += "\n### ğŸ³ ë ˆì‹œí”¼\n"
            for item in recipe_items[:2]:
                response += f"- {item.get('title', 'ë ˆì‹œí”¼')}\n"
                response += f"  {item.get('content', '')[:100]}...\n"
    
    return response


# ============================================================
# ê°•ì œ ì¢…ë£Œ ì‘ë‹µ
# ============================================================

def generate_forced_answer(state: AgentState) -> str:
    """ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ ì‹œ ê°•ì œ ì‘ë‹µ"""
    
    collected = []
    
    if state.get("llm_1st_response"):
        collected.append(f"ì´ˆì•ˆ: {state['llm_1st_response'][:200]}")
    
    for step in state.get("react_steps", [])[-3:]:
        if step.get("observation"):
            collected.append(f"- {step['observation'][:100]}")
    
    return f"""ìµœëŒ€ íƒìƒ‰ íšŸìˆ˜({state['max_iterations']}íšŒ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.

ìˆ˜ì§‘ëœ ì •ë³´:
{chr(10).join(collected) if collected else 'ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'}

ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""